{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import nanmean as np_nanmean\n",
    "from numpy import nanmedian as np_nanmedian\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Sheets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000001700\n",
      "1000001701\n",
      "1000001702\n",
      "strptime() argument 1 must be str, not datetime.datetime strptime() argument 1 must be str, not datetime.datetime\n",
      "strptime() argument 1 must be str, not datetime.datetime strptime() argument 1 must be str, not datetime.datetime\n",
      "strptime() argument 1 must be str, not datetime.datetime strptime() argument 1 must be str, not datetime.datetime\n",
      "1000001769\n",
      "1000001704\n",
      "1000001705\n",
      "1000001767\n",
      "1000001769\n",
      "1000001769\n",
      "1000001770\n",
      "1000001771\n",
      "1000001772\n",
      "1000001773\n",
      "1000001774\n",
      "1000001775\n",
      "1000001776\n",
      "1000001777\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "def date_parser(arr):\n",
    "    try:\n",
    "        return [datetime.datetime.strptime(x, \"%m-%d-%Y %H:%M:%S\") for x in arr]\n",
    "    except Exception as ex1:\n",
    "        try:\n",
    "            return [datetime.datetime.strptime(x, \"%m/%d/%Y %H:%M:%S\") for x in arr]\n",
    "        except Exception as ex2:\n",
    "            print(ex1, ex2)\n",
    "    return [None for x in arr]\n",
    "\n",
    "\n",
    "available_batches = list()\n",
    "for filename in glob.glob(\"./datasets/ODP*.xlsx\"):\n",
    "    df = pd.read_excel(filename, sheet_name=\"PO\")\n",
    "    available_batches.append(df[\"PO\"].iloc[0])\n",
    "    print(available_batches[-1])\n",
    "    os.makedirs(f\"./results/{available_batches[-1]}\", exist_ok=True)\n",
    "    df_BHV = pd.read_excel(\n",
    "        filename, sheet_name=\"BHV\", parse_dates=[0], date_parser=date_parser\n",
    "    )\n",
    "    df_BHV.columns = [\"Timestamp\"] + [f\"{x}_BHV\" for x in df_BHV.columns[1:]]\n",
    "    df_CFF = pd.read_excel(\n",
    "        filename, sheet_name=\"CFF\", parse_dates=[0], date_parser=date_parser\n",
    "    )\n",
    "    df_CFF.columns = [\"Timestamp\"] + [f\"{x}_CFF\" for x in df_CFF.columns[1:]]\n",
    "    df_NF = pd.read_excel(\n",
    "        filename, sheet_name=\"NF\", parse_dates=[0], date_parser=date_parser\n",
    "    )\n",
    "    df_NF.columns = [\"Timestamp\"] + [f\"{x}_NF\" for x in df_NF.columns[1:]]\n",
    "    df_EXT = pd.read_excel(\n",
    "        filename, sheet_name=\"EXT\", parse_dates=[0], date_parser=date_parser\n",
    "    )\n",
    "    df_EXT.columns = [\"Timestamp\"] + [f\"{x}_EXT\" for x in df_EXT.columns[1:]]\n",
    "\n",
    "    df_tmp1 = pd.merge(left=df_BHV, right=df_CFF, on=\"Timestamp\")\n",
    "    df_tmp2 = pd.merge(left=df_NF, right=df_EXT, on=\"Timestamp\")\n",
    "\n",
    "    df = pd.merge(left=df_tmp1, right=df_tmp2, on=\"Timestamp\")\n",
    "    df[\"Batch\"] = available_batches[-1]\n",
    "    # sort columns\n",
    "    df = df[[\"Batch\"] + [x for x in df.columns[:-1]]]\n",
    "    df.to_csv(\n",
    "        f\"./results/{available_batches[-1]}/{available_batches[-1]}.csv\", index=False\n",
    "    )\n",
    "\n",
    "with open(\"./results/batches.txt\", \"w\") as f:\n",
    "    for x in available_batches:\n",
    "        f.write(f\"{x}\\n\")\n",
    "print(len(available_batches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Batches in the Original Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_batches = list()\n",
    "with open(\"./results/batches.txt\", \"r\") as f:\n",
    "    available_batches.extend(int(x) for x in f.readlines())\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for batch in available_batches:\n",
    "    df = df.append(pd.read_csv(f\"./results/{batch}/{batch}.csv\"))\n",
    "df.to_csv(\"./results/all_batches.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy Statistical Measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping Strategies:\n",
      "Bin Time: 2\n",
      "Categorical: mode\n",
      "Numerical: mean\n",
      "1000001700\n",
      "1000001701\n",
      "1000001702\n",
      "single positional indexer is out-of-bounds\n",
      "1000001769\n",
      "1000001704\n",
      "1000001705\n",
      "Could not convert 21.42741775512695321.38013076782226621.33284378051757821.28555488586425821.2382678985595721.19098091125488321.14369201660156221.09640502929687521.04911804199218821.00182914733886720.9545421600341820.90725517272949220.85996627807617220.81267929077148420.76539230346679720.71810340881347720.6708164215087920.623529434204120.5984764099121120.5768699645996120.55526161193847720.53365325927734420.5120449066162120.49043655395507820.46882820129394520.44722175598144520.42561340332031220.4040050506591820.38239669799804720.36078834533691420.3391799926757820.3175735473632820.2959651947021520.27435684204101620.25274848937988320.2311401367187520.20953178405761720.18792533874511720.16631698608398420.1447086334228520.1231002807617220.10149192810058620.07988357543945320.05827713012695320.0366687774658220.01506042480468819.99345207214355519.97184371948242219.9502353668212919.9286289215087919.90702056884765619.88541221618652319.8638038635253919.84219551086425819.82058715820312519.79898071289062519.77737236022949219.7557640075683619.73415565490722719.71254730224609419.6909389495849619.6693325042724619.64772415161132819.62611579895019519.60450744628906219.5828990936279319.5612926483154319.53968429565429719.51807594299316419.4964675903320319.474859237670919.45325088500976619.43164443969726619.41003608703613319.38842773437519.36681938171386719.34521102905273419.323602676391619.301996231079119.2803878784179719.25877952575683619.23717117309570319.2155628204345719.19395446777343819.17234802246093819.15073966979980519.12913131713867219.1075229644775419.08591461181640619.06430625915527319.04269981384277319.0210914611816418.99948310852050818.97787475585937518.95626640319824218.9346580505371118.9130516052246118.89144325256347718.86983489990234418.8482265472412118.82661819458007818.80500984191894518.78340339660644518.76179504394531218.7401866912841818.71857833862304718.69696998596191418.6753616333007818.6537551879882818.6321468353271518.61053848266601618.58893013000488318.5673217773437518.54571342468261718.52410697937011718.50249862670898418.4808902740478518.4592819213867218.43767356872558618.416065216064453 to numeric\n",
      "1000001767\n",
      "1000001769\n",
      "1000001769\n",
      "1000001770\n",
      "1000001771\n",
      "1000001772\n",
      "1000001773\n",
      "1000001774\n",
      "1000001775\n",
      "1000001776\n",
      "1000001777\n",
      "single positional indexer is out-of-bounds\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "HYPER_bin_time = 2  # hours\n",
    "HYPER_categ_agg = \"mode\"  # possible values: mode, last\n",
    "HYPER_numer_agg = \"mean\"  # possible values: mean, median, last\n",
    "print(\n",
    "    f\"Grouping Strategies:\\nBin Time: {HYPER_bin_time}\\nCategorical: {HYPER_categ_agg}\\nNumerical: {HYPER_numer_agg}\"\n",
    ")\n",
    "\n",
    "\n",
    "def custom_mode(x):\n",
    "    m = pd.Series.mode(x)\n",
    "    return m.iloc[0] if not m.empty else None\n",
    "\n",
    "\n",
    "def get_agg_method(x):\n",
    "    if x == \"last\" or x == \"mean\":\n",
    "        return x\n",
    "    elif x == \"mode\":\n",
    "        return custom_mode\n",
    "    elif x == \"median\":\n",
    "        return np_nanmedian\n",
    "\n",
    "\n",
    "previous_available_batches = list()\n",
    "with open(\"./results/batches.txt\", \"r\") as f:\n",
    "    previous_available_batches.extend(int(x) for x in f.readlines())\n",
    "\n",
    "available_batches = list()\n",
    "for filename in glob.glob(\"./datasets/ODP*.xlsx\"):\n",
    "    df = pd.read_excel(filename, sheet_name=\"PO\")\n",
    "    batch = df[\"PO\"].iloc[0]\n",
    "    print(batch)\n",
    "    if batch in previous_available_batches:\n",
    "        df = pd.read_csv(f\"./results/{batch}/{batch}.csv\", parse_dates=[1])\n",
    "        try:\n",
    "            start_time = df[\"Timestamp\"].iloc[0]\n",
    "            # compute elapsed minutes since start\n",
    "            elapsed_minutes = list()\n",
    "            for tup in df.itertuples():\n",
    "                elapsed_minutes.append(\n",
    "                    round((tup.Timestamp - start_time).total_seconds() / 60, 2)\n",
    "                )\n",
    "            df[\"Elapsed Minutes\"] = elapsed_minutes\n",
    "\n",
    "            # remove datetime as no longer needed\n",
    "            df.drop([\"Timestamp\"], axis=1, inplace=True)\n",
    "\n",
    "            # bin elapsed minutes\n",
    "            df.loc[:, \"Elapsed Minutes\"] = (\n",
    "                df[\"Elapsed Minutes\"]\n",
    "                // (HYPER_bin_time * 60)  # floor division by X hours (obtain hours)\n",
    "                * (HYPER_bin_time * 60)  # multiply again to obtain minutes\n",
    "            )\n",
    "            # extract categorical/numerical variables\n",
    "            categorical_vars = list()\n",
    "            numerical_vars = [\n",
    "                x\n",
    "                for x in df.columns\n",
    "                if x != \"Elapsed Minutes\" and x not in categorical_vars\n",
    "            ]\n",
    "            agg_dict = {x: get_agg_method(HYPER_categ_agg) for x in categorical_vars}\n",
    "            agg_dict.update(\n",
    "                {x: get_agg_method(HYPER_numer_agg) for x in numerical_vars}\n",
    "            )\n",
    "            # apply aggregation methods\n",
    "            df_tmp = df.groupby(by=[\"Elapsed Minutes\"]).agg(agg_dict)\n",
    "            df_tmp.reset_index(inplace=True)\n",
    "            df_tmp[\"Batch\"] = batch\n",
    "            # sort columns\n",
    "            df_tmp = df_tmp[[\"Batch\", \"Elapsed Minutes\"] + [x for x in df.columns[:-2]]]\n",
    "            df_tmp.to_csv(\n",
    "                f\"./results/{batch}/{batch}_binned.csv\",\n",
    "                index=False,\n",
    "            )\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "        else:\n",
    "            available_batches.append(batch)\n",
    "\n",
    "with open(\"./results/batches_binned.txt\", \"w\") as f:\n",
    "    for x in available_batches:\n",
    "        f.write(f\"{x}\\n\")\n",
    "print(len(available_batches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_batches = list()\n",
    "with open(\"./results/batches_binned.txt\", \"r\") as f:\n",
    "    available_batches.extend(int(x) for x in f.readlines())\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for batch in available_batches:\n",
    "    df = df.append(pd.read_csv(f\"./results/{batch}/{batch}_binned.csv\"))\n",
    "df.to_csv(\"./results/all_batches_binned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Datasets into Train-Test and Impute Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation Strategies:\n",
      "Categorical: mode\n",
      "Numerical: median\n"
     ]
    }
   ],
   "source": [
    "HYPER_categ_fillna = \"mode\"  # possible values: mode\n",
    "HYPER_numer_fillna = \"median\"  # possible values: mean, median\n",
    "print(\n",
    "    f\"Imputation Strategies:\\nCategorical: {HYPER_categ_fillna}\\nNumerical: {HYPER_numer_fillna}\"\n",
    ")\n",
    "\n",
    "\n",
    "def custom_mode(x):\n",
    "    m = pd.Series.mode(x)\n",
    "    return m.iloc[0] if not m.empty else None\n",
    "\n",
    "\n",
    "def get_fillna_method(x):\n",
    "    if x == \"mean\":\n",
    "        return np_nanmean\n",
    "    elif x == \"median\":\n",
    "        return np_nanmedian\n",
    "    elif x == \"mode\":\n",
    "        return custom_mode\n",
    "\n",
    "\n",
    "available_batches = list()\n",
    "with open(\"./results/batches_binned.txt\", \"r\") as f:\n",
    "    available_batches.extend(int(x) for x in f.readlines())\n",
    "train_seq_ids, test_seq_ids = model_selection.train_test_split(\n",
    "    available_batches, train_size=0.75, random_state=666\n",
    ")\n",
    "os.makedirs(\"./results/splits\", exist_ok=True)\n",
    "df = pd.read_csv(\"./results/all_batches_binned.csv\")\n",
    "df.sort_values(by=\"Batch\", inplace=True)\n",
    "df_targets = pd.read_excel(\n",
    "    \"./datasets/produzione_CStOA_2021_ed12.xlsx\", sheet_name=\"dati-produzione\", header=1\n",
    ")\n",
    "df_targets.sort_values(by=\"O.D.P.\", inplace=True)\n",
    "\n",
    "df_train = df[df[\"Batch\"].isin(train_seq_ids)]\n",
    "df_test = df[df[\"Batch\"].isin(test_seq_ids)]\n",
    "\n",
    "X_train = df_train.copy()\n",
    "# remove duplicates of the same batch\n",
    "y_train = df_targets[df_targets[\"O.D.P.\"].isin(train_seq_ids)][\"Resa\"]\n",
    "y_train.to_csv(\"./results/splits/y_train.csv\", index=False)\n",
    "\n",
    "X_test = df_test.copy()\n",
    "# remove duplicates of the same Batch\n",
    "y_test = df_targets[df_targets[\"O.D.P.\"].isin(test_seq_ids)][\"Resa\"]\n",
    "y_test.to_csv(\"./results/splits/y_test.csv\", index=False)\n",
    "\n",
    "# extract excluded/categorical/numerical variables\n",
    "excluded_vars = (\n",
    "    \"Batch\",\n",
    "    \"Elapsed Minutes\",\n",
    ")\n",
    "categorical_vars = list()\n",
    "numerical_vars = [\n",
    "    x for x in df.columns if x not in excluded_vars and x not in categorical_vars\n",
    "]\n",
    "# create dictionary for fill-forwarding with the appropriate method\n",
    "fillna_dict = {\n",
    "    x: get_fillna_method(HYPER_categ_fillna)(X_train[x]) for x in categorical_vars\n",
    "}\n",
    "fillna_dict.update(\n",
    "    {x: get_fillna_method(HYPER_numer_fillna)(X_train[x]) for x in numerical_vars}\n",
    ")\n",
    "\n",
    "X_train.fillna(fillna_dict, inplace=True)\n",
    "X_train.to_csv(\"./results/splits/X_train.csv\", index=False)\n",
    "\n",
    "# impute test with values obtained from train\n",
    "X_test.fillna(fillna_dict, inplace=True)\n",
    "X_test.to_csv(\"./results/splits/X_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearise Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_max_pad = 15\n",
    "\n",
    "for dataset in (\"train\", \"test\"):\n",
    "    X = pd.read_csv(f\"./results/splits/X_{dataset}.csv\")\n",
    "\n",
    "    new_columns = list()\n",
    "    for i in range(HYPER_max_pad):\n",
    "        new_columns.extend([f\"{x}_{i}\" for x in X.columns])\n",
    "\n",
    "    grouped = X.groupby(X[\"Batch\"])\n",
    "\n",
    "    rows = list()\n",
    "    for k, group in grouped.groups.items():\n",
    "        # add row containing both real rows and zero rows\n",
    "        if len(group) > HYPER_max_pad:\n",
    "            group = group[:HYPER_max_pad]\n",
    "        X_sub = X.loc[group]\n",
    "\n",
    "        row = list()\n",
    "        for i in range(len(group)):\n",
    "            row.extend(X_sub.iloc[i].tolist())\n",
    "        for i in range(HYPER_max_pad - len(group)):\n",
    "            row.extend([0] * len(X.columns))\n",
    "        rows.append(row)\n",
    "\n",
    "    new_X = pd.DataFrame(data=rows, columns=new_columns)\n",
    "    new_X.to_csv(f\"./results/splits/X_{dataset}_padded_translated.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bddf87d3d8ffc9ffe643d1c63ef98549d54d18316b6f1377644eea2ca2a14d20"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
