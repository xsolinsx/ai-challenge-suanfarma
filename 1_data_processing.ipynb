{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34de7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# importing user-defined functions from udf_eda.py\n",
    "from src import udf_eda as udf\n",
    "\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ae95b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [03:25<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following batches have incompatible data:  ['100001576', '100001778']\n",
      "# of batches read:  101\n",
      "Missing batches, if any: {'', 'FF_N'}\n",
      "How many NaN values exist in the merged data:  8936962\n",
      "Shape of the merged data:  (87436, 364)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following batches have incompatible data:  []\n",
      "# of batches read:  1\n",
      "Missing batches, if any: {''}\n",
      "How many NaN values exist in the merged data:  1187\n",
      "Shape of the merged data:  (1190, 169)\n",
      "#Available Batches: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_names = [x.replace(\"\\\\\", \"/\") for x in glob.glob(\"./data/datasets/**/ODP *.xlsx\")]\n",
    "df_BHV_CFF_orig, df_NF_orig, df_EXT_orig, df_batches_orig = udf.read_bind(file_names)\n",
    "_, _, _, df_features_orig = udf.read_bind(\n",
    "    [x.replace(\"\\\\\", \"/\") for x in glob.glob(\"./data/ODP *.xlsx\")]\n",
    ")\n",
    "print(f\"#Available Batches: {len(file_names)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13da5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BHV_CFF = df_BHV_CFF_orig.copy()\n",
    "df_NF = df_NF_orig.copy()\n",
    "df_EXT = df_EXT_orig.copy()\n",
    "df_batches = df_batches_orig.copy()\n",
    "df_features = df_features_orig.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f9783e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of NaN in the data: 8936962\n",
      "Dataframe shape: (87436, 364)\n"
     ]
    }
   ],
   "source": [
    "df_features.drop(\"TAG\", axis=1, inplace=True)\n",
    "# remove blank columns that have been named \"Unnamed*\" by pandas when reading the files\n",
    "df_batches.drop(\n",
    "    [x for x in df_batches.columns if x.startswith(\"Unnamed\")], inplace=True\n",
    ")\n",
    "print(f\"Amount of NaN in the data: {df_batches.isna().sum().sum()}\")\n",
    "print(f\"Dataframe shape: {df_batches.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7daac5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of NaN in the data: 102460\n",
      "Dataframe shape: (86278, 168)\n",
      "Columns with NA values: ['118FIC606', '118ZLH303', '118LS960', '118LS690', '118ZLL417', '118ZLL427', '118ZLL437', '118ZLL447', '118FI912', '118FI913', '118PI629', '118PI628', '118PI924', '118PI964', '118PI925', '118PI639', '118PI638', '118PI934', '118PI952', '118PI935', '118PI649', '118PI648', '118PI944', '118PI953', '118PI945']\n"
     ]
    }
   ],
   "source": [
    "# restrict columns to the ones selected by the company\n",
    "df_batches = df_batches[df_features.columns.tolist()]\n",
    "# remove specific batch\n",
    "df_batches = df_batches[df_batches[\"id\"] != \"100001510\"]\n",
    "print(f\"Amount of NaN in the data: {df_batches.isna().sum().sum()}\")\n",
    "print(f\"Dataframe shape: {df_batches.shape}\")\n",
    "print(f\"Columns with NA values: {df_batches.columns[df_batches.isna().any()].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a44b0f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (86278, 143)\n"
     ]
    }
   ],
   "source": [
    "# drop columns containing NA values\n",
    "df_batches.drop(\n",
    "    df_batches.columns[df_batches.isna().any()].tolist(), axis=1, inplace=True\n",
    ")\n",
    "print(f\"Dataframe shape: {df_batches.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35c54b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (86278, 123)\n"
     ]
    }
   ],
   "source": [
    "# drop \"object\" (str) columns\n",
    "df_batches.drop(\n",
    "    df_batches.select_dtypes(include=[\"O\"]).columns.tolist(), axis=1, inplace=True\n",
    ")\n",
    "# drop columns starting with 164\n",
    "df_batches.drop(df_batches.filter(regex=\"^164\").columns.tolist(), axis=1, inplace=True)\n",
    "print(f\"Dataframe shape: {df_batches.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd3d5641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FI679P', 'PI678', 'TI670', 'FI667', 'FI654', 'TI684', 'TI650', 'FI652', 'CAL4552', 'CAL4551', 'FI656', 'FI666'}\n",
      "{'FI679P': ['158'], 'PI678': ['158'], 'TI670': ['158'], 'FI667': ['158', '159', '160', '161', '162', '163'], 'FI654': ['158', '159', '160', '161', '162', '163'], 'TI684': ['158'], 'TI650': ['158', '159', '160', '161', '162', '163', '165'], 'FI652': ['158', '159', '160', '161', '162', '163'], 'CAL4552': ['158', '159', '160', '161', '162', '163', '165'], 'CAL4551': ['158', '159', '160', '161', '162', '163', '165'], 'FI656': ['158', '159', '161', '162', '163'], 'FI666': ['158', '159', '161', '162', '163']}\n",
      "Dataframe shape pre-drop: (86278, 135)\n",
      "Dataframe shape post-drop: (86278, 82)\n"
     ]
    }
   ],
   "source": [
    "# check which sensors are available for units from 158 to 165\n",
    "units_158_to_165_sensors = set()\n",
    "for unit in range(158, 166):\n",
    "    for x in df_batches.filter(regex=f\"^{unit}\").columns.tolist():\n",
    "        units_158_to_165_sensors.add(x.replace(f\"{unit}\", \"\"))\n",
    "print(units_158_to_165_sensors)\n",
    "\n",
    "# check which units are available for each sensor\n",
    "units_per_sensor = dict()\n",
    "for sensor in units_158_to_165_sensors:\n",
    "    units_per_sensor[sensor] = [\n",
    "        x.replace(sensor, \"\") for x in df_batches.filter(regex=sensor).columns\n",
    "    ]\n",
    "print(units_per_sensor)\n",
    "\n",
    "\n",
    "# keep only valid sensors according to TI\n",
    "def filtering(row, sensor, units):\n",
    "    i = 0\n",
    "    row_sum = 0\n",
    "    for unit in units:\n",
    "        if row[f\"{unit}TI650\"] < 30:\n",
    "            i += 1\n",
    "            row_sum += row[f\"{unit}{sensor}\"]\n",
    "    if i > 0:\n",
    "        return row_sum / i\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "for sensor, units in units_per_sensor.items():\n",
    "    df_batches[sensor] = df_batches.agg(filtering, axis=1, sensor=sensor, units=units)\n",
    "print(f\"Dataframe shape pre-drop: {df_batches.shape}\")\n",
    "\n",
    "# drop units measurements as we have the mean ones\n",
    "for unit in range(158, 166):\n",
    "    df_batches.drop(\n",
    "        df_batches.filter(regex=f\"^{unit}\").columns.tolist(), axis=1, inplace=True\n",
    "    )\n",
    "print(f\"Dataframe shape post-drop: {df_batches.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9085b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AI612', 'TI660', 'FI689A', 'TI656', 'AI673A', 'TI607', 'FI657', 'TI664', 'FI677', 'FI681', 'AI610', 'PI650', 'FI693A', 'LI606', 'FI653', 'AI672', 'AI611', 'FI673', 'FI685A', 'FI665', 'TI652', 'AI613', 'AI677A', 'FI669', 'AI674A'}\n",
      "Dataframe shape pre-drop: (86278, 107)\n",
      "Dataframe shapepost-drop: (86278, 80)\n"
     ]
    }
   ],
   "source": [
    "# check which sensors are available for units 107 and 108\n",
    "units_107_108_sensors = set()\n",
    "for unit in range(107, 109):\n",
    "    for x in df_batches.filter(regex=f\"^{unit}\").columns.tolist():\n",
    "        units_107_108_sensors.add(x.replace(f\"{unit}\", \"\"))\n",
    "print(units_107_108_sensors)\n",
    "\n",
    "\n",
    "# keep only valid sensors according to PI\n",
    "def filtering(row, sensor, units):\n",
    "    i = 0\n",
    "    row_sum = 0\n",
    "    for unit in units:\n",
    "        if f\"{unit}{sensor}\" in row and row[f\"{unit}PI650\"] > 10:\n",
    "            i += 1\n",
    "            row_sum += row[f\"{unit}{sensor}\"]\n",
    "    if i > 0:\n",
    "        return row_sum / i\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "for sensor in units_107_108_sensors:\n",
    "    df_batches[sensor] = df_batches.agg(\n",
    "        filtering, axis=1, sensor=sensor, units=[107, 108]\n",
    "    )\n",
    "print(f\"Dataframe shape pre-drop: {df_batches.shape}\")\n",
    "\n",
    "# drop units measurements as we have the mean ones\n",
    "for unit in range(107, 109):\n",
    "    df_batches.drop(\n",
    "        df_batches.filter(regex=f\"^{unit}\").columns.tolist(), axis=1, inplace=True\n",
    "    )\n",
    "print(f\"Dataframe shapepost-drop: {df_batches.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c202b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ext = [\n",
    "    \"118PI946\",\n",
    "    \"118PI628\",\n",
    "    \"118PI649\",\n",
    "    \"113LI682.1\",\n",
    "    \"118PI952\",\n",
    "    \"118PI953\",\n",
    "    \"118PI964\",\n",
    "    \"118PI925\",\n",
    "    \"118PI945\",\n",
    "    \"118PI648\",\n",
    "    \"118FI912\",\n",
    "    \"118PI934\",\n",
    "    \"118PI638\",\n",
    "    \"118PI935\",\n",
    "    \"118PI629\",\n",
    "    \"118PI924\",\n",
    "    \"118PI639\",\n",
    "    \"118FI913\",\n",
    "    \"113LI682\",\n",
    "    \"118PI944\",\n",
    "    \"118TI977\",\n",
    "    \"164FI656\",\n",
    "    \"164FI666\",\n",
    "    \"164FI667\",\n",
    "    \"118PI936\",\n",
    "    \"118ZLL417\",\n",
    "    \"118ZLL427\",\n",
    "    \"118AI641\",\n",
    "    \"118PI974\",\n",
    "    \"118SI643\",\n",
    "    \"163TIC650\",\n",
    "    \"118SI633\",\n",
    "    \"118ZLL437\",\n",
    "    \"118PI927\",\n",
    "    \"118PI916\",\n",
    "    \"118SI613\",\n",
    "    \"118LS690\",\n",
    "    \"118ZLH303\",\n",
    "    \"164PI653\",\n",
    "    \"163PI663\",\n",
    "    \"118LS960\",\n",
    "    \"118PI917\",\n",
    "    \"890PI610\",\n",
    "    \"118SI623\",\n",
    "    \"118PI937\",\n",
    "    \"118PI951\",\n",
    "    \"118ZLL447\",\n",
    "    \"163PI655\",\n",
    "    \"118FIC606\",\n",
    "    \"118PI947\",\n",
    "    \"164PI655\",\n",
    "    \"118AI631\",\n",
    "    \"118PI954\",\n",
    "    \"118PI984\",\n",
    "    \"164PI651\",\n",
    "    \"118AI621\",\n",
    "    \"118PI926\",\n",
    "    \"118AI611\",\n",
    "    \"164PI663\",\n",
    "    \"163TI650\",\n",
    "    \"164FI652\",\n",
    "    \"164FI654\",\n",
    "]\n",
    "# drop specific EXT columns\n",
    "df_EXT.drop(list_ext, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c894fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace invalid EXT data with 0\n",
    "df_batches.loc[\n",
    "    df_batches[\"118CV502\"] <= 0,\n",
    "    df_batches[df_EXT.columns[2:].tolist()].columns.tolist(),\n",
    "] = 0\n",
    "df_batches.loc[\n",
    "    df_batches[\"118CV501\"] <= 0,\n",
    "    df_batches[df_EXT.columns[2:].tolist()].columns.tolist(),\n",
    "] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7750925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract yield from produzione_CStOA_2021_ed12.xlsx\n",
    "df_yield = pd.read_excel(\n",
    "    \"./data/datasets/produzione_CStOA_2021_ed12.xlsx\",\n",
    "    sheet_name=\"dati-produzione\",\n",
    "    header=1,\n",
    ")\n",
    "df_yield = df_yield[[\"O.D.P.\", \"Resa\"]]\n",
    "df_yield.dropna(axis=0, how=\"any\", inplace=True)\n",
    "df_yield[\"O.D.P.\"] = df_yield[\"O.D.P.\"].astype(int).astype(\"string\")\n",
    "df_yield.columns = [\"id\", \"result\"]\n",
    "df_yield.result = round(df_yield.result, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09dbb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only meaningful part of id\n",
    "df_batches[\"id\"] = df_batches[\"id\"].str[-4:]\n",
    "df_yield[\"id\"] = df_yield[\"id\"].str[-4:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b1decb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (85088, 81)\n"
     ]
    }
   ],
   "source": [
    "# merge target with data\n",
    "df_batches = df_yield.merge(df_batches, how=\"inner\")\n",
    "print(f\"Dataframe shape: {df_batches.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b243ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add start_date, end_date, processing_time_mins and timestamp_index columns\n",
    "df_begin_end = (\n",
    "    df_batches.groupby([\"id\"])[\"timeseries\"].agg([\"min\", \"max\"]).reset_index()\n",
    ")\n",
    "df_begin_end.columns = [\"id\", \"start_date\", \"end_date\"]\n",
    "df_begin_end[\"processing_time_mins\"] = (\n",
    "    (df_begin_end[\"end_date\"] - df_begin_end[\"start_date\"]) / pd.Timedelta(minutes=5)\n",
    ") + 1\n",
    "df_batches = df_begin_end.merge(df_batches, how=\"right\")\n",
    "df_batches.insert(5, \"timestamp_index\", df_batches.groupby(\"id\").cumcount())\n",
    "# adjust DIAFTOTALE values\n",
    "df_batches.loc[df_batches[\"DIAFTOTALE\"] > 3, \"DIAFTOTALE\"] = 2.59\n",
    "df_batches.loc[df_batches[\"DIAFTOTALE\"] < -1, \"DIAFTOTALE\"] = -0.4\n",
    "# insert the progress column which measures progress in percentage terms\n",
    "df_batches.insert(\n",
    "    6,\n",
    "    \"progress_perc\",\n",
    "    round(\n",
    "        (df_batches.timestamp_index / df_batches.processing_time_mins) * 100, 0\n",
    "    ).astype(int),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0f6f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group every batch into its own progress_perc with mean\n",
    "df_grouped = (\n",
    "    df_batches.groupby(\n",
    "        [\n",
    "            \"id\",\n",
    "            \"start_date\",\n",
    "            \"end_date\",\n",
    "            \"processing_time_mins\",\n",
    "            \"result\",\n",
    "            \"progress_perc\",\n",
    "        ]\n",
    "    )[df_batches.columns[8:].tolist()]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# compute stats using pandas describe\n",
    "df_stats_describe = df_grouped.groupby([\"id\"]).describe()\n",
    "df_stats_describe.columns = [\n",
    "    \"_\".join(col).strip() for col in df_stats_describe.columns.values\n",
    "]\n",
    "df_stats_describe.reset_index(inplace=True)\n",
    "df_stats_describe = df_stats_describe[\n",
    "    df_stats_describe.columns.drop(\n",
    "        df_stats_describe.filter(regex=\"_count\").columns.tolist()\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e614b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with suspected change points\n",
    "change_point_vars = df_grouped.columns[5:].insert(0, \"id\")\n",
    "df_change_points = df_grouped[\n",
    "    df_grouped[\"progress_perc\"].isin([0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
    "]\n",
    "df_change_points = df_change_points[change_point_vars]\n",
    "df_melt = pd.melt(df_change_points, id_vars=[\"id\", \"progress_perc\"])\n",
    "df_melt[\"variable\"] = (\n",
    "    df_melt[\"variable\"].astype(str) + \"_\" + df_melt[\"progress_perc\"].astype(str)\n",
    ")\n",
    "df_melt.drop(columns=[\"progress_perc\"], inplace=True)\n",
    "df_change_points = df_melt.pivot(index=\"id\", columns=\"variable\", values=\"value\")\n",
    "df_change_points.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "676a5020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (92, 1484)\n",
      "Amount of NaN in the data: 0\n"
     ]
    }
   ],
   "source": [
    "df_final = df_stats_describe.merge(df_change_points, on=\"id\")\n",
    "df_final.drop(\n",
    "    [\n",
    "        \"processing_time_mins_mean\",\n",
    "        \"processing_time_mins_std\",\n",
    "        \"processing_time_mins_min\",\n",
    "        \"processing_time_mins_25%\",\n",
    "        \"processing_time_mins_50%\",\n",
    "        \"processing_time_mins_75%\",\n",
    "        \"processing_time_mins_max\",\n",
    "        \"result_mean\",\n",
    "        \"result_std\",\n",
    "        \"result_min\",\n",
    "        \"result_25%\",\n",
    "        \"result_50%\",\n",
    "        \"result_75%\",\n",
    "        \"result_max\",\n",
    "        \"progress_perc_mean\",\n",
    "        \"progress_perc_std\",\n",
    "        \"progress_perc_min\",\n",
    "        \"progress_perc_25%\",\n",
    "        \"progress_perc_50%\",\n",
    "        \"progress_perc_75%\",\n",
    "        \"progress_perc_max\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "# re-add yield\n",
    "df_final = df_yield.merge(df_final, how=\"inner\")\n",
    "df_final.dropna(axis=0, inplace=True)\n",
    "print(f\"Dataframe shape: {df_final.shape}\")\n",
    "print(f\"Amount of NaN in the data: {df_final.isna().sum().sum()}\")\n",
    "df_final.to_csv(\"./data/suanfarma_train_data.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
