{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import glob\r\n",
    "import json\r\n",
    "import os\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "from numpy import nanmean as np_nanmean\r\n",
    "from numpy import nanmedian as np_nanmedian\r\n",
    "from sklearn import model_selection\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GroupBy Statistical Measures\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "HYPER_bin_time = 2  # hours\r\n",
    "HYPER_categ_agg = \"mode\"  # possible values: mode, last\r\n",
    "HYPER_numer_agg = \"mean\"  # possible values: mean, median, last\r\n",
    "print(\r\n",
    "    f\"Grouping Strategies:\\nBin Time: {HYPER_bin_time}\\nCategorical: {HYPER_categ_agg}\\nNumerical: {HYPER_numer_agg}\"\r\n",
    ")\r\n",
    "\r\n",
    "\r\n",
    "def custom_mode(x):\r\n",
    "    m = pd.Series.mode(x)\r\n",
    "    return m.iloc[0] if not m.empty else None\r\n",
    "\r\n",
    "\r\n",
    "def get_agg_method(x):\r\n",
    "    if x == \"last\" or x == \"mean\":\r\n",
    "        return x\r\n",
    "    elif x == \"mode\":\r\n",
    "        return custom_mode\r\n",
    "    elif x == \"median\":\r\n",
    "        return np_nanmedian\r\n",
    "\r\n",
    "\r\n",
    "os.makedirs(\"./results\", exist_ok=True)\r\n",
    "available_batches = list()\r\n",
    "for filename in glob.glob(f\"./datasets/ODP*.xlsx\"):\r\n",
    "    df = pd.read_excel(filename, sheet_name=\"PO\")\r\n",
    "    available_batches.append(df[\"PO\"].iloc[0])\r\n",
    "    os.makedirs(f\"./results/{available_batches[-1]}\", exist_ok=True)\r\n",
    "    df_BHV = pd.read_excel(filename, sheet_name=\"BHV\", parse_dates=[0])\r\n",
    "    df_BHV.columns = [\"Timestamp\"] + [f\"{x}_BHV\" for x in df_BHV.columns[1:]]\r\n",
    "    df_CFF = pd.read_excel(filename, sheet_name=\"CFF\", parse_dates=[0])\r\n",
    "    df_CFF.columns = [\"Timestamp\"] + [f\"{x}_CFF\" for x in df_CFF.columns[1:]]\r\n",
    "    df_NF = pd.read_excel(filename, sheet_name=\"NF\", parse_dates=[0])\r\n",
    "    df_NF.columns = [\"Timestamp\"] + [f\"{x}_NF\" for x in df_NF.columns[1:]]\r\n",
    "    df_EXT = pd.read_excel(filename, sheet_name=\"EXT\", parse_dates=[0])\r\n",
    "    df_EXT.columns = [\"Timestamp\"] + [f\"{x}_EXT\" for x in df_EXT.columns[1:]]\r\n",
    "\r\n",
    "    df_tmp1 = pd.merge(left=df_BHV, right=df_CFF, on=\"Timestamp\")\r\n",
    "    df_tmp2 = pd.merge(left=df_NF, right=df_EXT, on=\"Timestamp\")\r\n",
    "\r\n",
    "    df = pd.merge(left=df_tmp1, right=df_tmp2, on=\"Timestamp\")\r\n",
    "    start_time = df[\"Timestamp\"].iloc[0]\r\n",
    "    # compute elapsed minutes since start\r\n",
    "    elapsed_minutes = list()\r\n",
    "    for tup in df.itertuples():\r\n",
    "        elapsed_minutes.append(\r\n",
    "            round((tup.Timestamp - start_time).total_seconds() / 60, 2)\r\n",
    "        )\r\n",
    "    df[\"Elapsed Minutes\"] = elapsed_minutes\r\n",
    "\r\n",
    "    # remove datetime as no longer needed\r\n",
    "    df.drop([\"Timestamp\"], axis=1, inplace=True)\r\n",
    "\r\n",
    "    # bin elapsed minutes\r\n",
    "    df.loc[:, \"Elapsed Minutes\"] = (\r\n",
    "        df[\"Elapsed Minutes\"]\r\n",
    "        // (HYPER_bin_time * 60)  # floor division by X hours (obtain hours)\r\n",
    "        * (HYPER_bin_time * 60)  # multiply again to obtain minutes\r\n",
    "    )\r\n",
    "    # extract categorical/numerical variables\r\n",
    "    categorical_vars = list()\r\n",
    "    numerical_vars = [\r\n",
    "        x for x in df.columns if x != \"Elapsed Minutes\" and x not in categorical_vars\r\n",
    "    ]\r\n",
    "    agg_dict = {x: get_agg_method(HYPER_categ_agg) for x in categorical_vars}\r\n",
    "    agg_dict.update({x: get_agg_method(HYPER_numer_agg) for x in numerical_vars})\r\n",
    "    # apply aggregation methods\r\n",
    "    df_tmp = df.groupby(by=[\"Elapsed Minutes\"]).agg(agg_dict)\r\n",
    "    df_tmp.reset_index(inplace=True)\r\n",
    "    df_tmp[\"Batch\"] = available_batches[-1]\r\n",
    "    # sort columns\r\n",
    "    df_tmp = df_tmp[[\"Batch\", \"Elapsed Minutes\"] + [x for x in df.columns[:-2]]]\r\n",
    "    df_tmp.to_csv(\r\n",
    "        f\"./results/{available_batches[-1]}/{available_batches[-1]}.csv\", index=False\r\n",
    "    )\r\n",
    "\r\n",
    "with open(\"./results/batches.txt\", \"w\") as f:\r\n",
    "    for x in available_batches:\r\n",
    "        f.write(f\"{x}\\n\")\r\n",
    "print(len(available_batches))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Grouping Strategies:\n",
      "Bin Time: 2\n",
      "Categorical: mode\n",
      "Numerical: mean\n",
      "8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge Everything\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "available_batches = list()\r\n",
    "with open(\"./results/batches.txt\", \"r\") as f:\r\n",
    "    available_batches.extend(int(x) for x in f.readlines())\r\n",
    "\r\n",
    "df = pd.DataFrame()\r\n",
    "for batch in available_batches:\r\n",
    "    df = df.append(pd.read_csv(f\"./results/{batch}/{batch}.csv\"))\r\n",
    "df.to_csv(\"./results/all_batches.csv\", index=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split Datasets into Train-Test and Impute Missing Values\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "HYPER_categ_fillna = \"mode\"  # possible values: mode\r\n",
    "HYPER_numer_fillna = \"median\"  # possible values: mean, median\r\n",
    "print(\r\n",
    "    f\"Imputation Strategies:\\nCategorical: {HYPER_categ_fillna}\\nNumerical: {HYPER_numer_fillna}\"\r\n",
    ")\r\n",
    "\r\n",
    "\r\n",
    "def custom_mode(x):\r\n",
    "    m = pd.Series.mode(x)\r\n",
    "    return m.iloc[0] if not m.empty else None\r\n",
    "\r\n",
    "\r\n",
    "def get_fillna_method(x):\r\n",
    "    if x == \"mean\":\r\n",
    "        return np_nanmean\r\n",
    "    elif x == \"median\":\r\n",
    "        return np_nanmedian\r\n",
    "    elif x == \"mode\":\r\n",
    "        return custom_mode\r\n",
    "\r\n",
    "\r\n",
    "available_batches = list()\r\n",
    "with open(\"./results/batches.txt\", \"r\") as f:\r\n",
    "    available_batches.extend(int(x) for x in f.readlines())\r\n",
    "train_seq_ids, test_seq_ids = model_selection.train_test_split(\r\n",
    "    available_batches, train_size=0.75, random_state=666\r\n",
    ")\r\n",
    "os.makedirs(\"./results/splits\", exist_ok=True)\r\n",
    "df = pd.read_csv(\"./results/all_batches.csv\")\r\n",
    "df.sort_values(by=\"Batch\", inplace=True)\r\n",
    "df_targets = pd.read_excel(\r\n",
    "    \"./datasets/produzione_CStOA_2021_ed12.xlsx\", sheet_name=\"dati-produzione\", header=1\r\n",
    ")\r\n",
    "df_targets.sort_values(by=\"O.D.P.\", inplace=True)\r\n",
    "\r\n",
    "df_train = df[df[\"Batch\"].isin(train_seq_ids)]\r\n",
    "df_test = df[df[\"Batch\"].isin(test_seq_ids)]\r\n",
    "\r\n",
    "X_train = df_train.copy()\r\n",
    "# remove duplicates of the same batch\r\n",
    "y_train = df_targets[df_targets[\"O.D.P.\"].isin(train_seq_ids)][\"Resa\"]\r\n",
    "y_train.to_csv(\"./results/splits/y_train.csv\", index=False)\r\n",
    "\r\n",
    "X_test = df_test.copy()\r\n",
    "# remove duplicates of the same Batch\r\n",
    "y_test = df_targets[df_targets[\"O.D.P.\"].isin(test_seq_ids)][\"Resa\"]\r\n",
    "y_test.to_csv(\"./results/splits/y_test.csv\", index=False)\r\n",
    "\r\n",
    "# extract excluded/categorical/numerical variables\r\n",
    "excluded_vars = (\r\n",
    "    \"Batch\",\r\n",
    "    \"Elapsed Minutes\",\r\n",
    ")\r\n",
    "categorical_vars = list()\r\n",
    "numerical_vars = [\r\n",
    "    x for x in df.columns if x not in excluded_vars and x not in categorical_vars\r\n",
    "]\r\n",
    "# create dictionary for fill-forwarding with the appropriate method\r\n",
    "fillna_dict = {\r\n",
    "    x: get_fillna_method(HYPER_categ_fillna)(X_train[x]) for x in categorical_vars\r\n",
    "}\r\n",
    "fillna_dict.update(\r\n",
    "    {x: get_fillna_method(HYPER_numer_fillna)(X_train[x]) for x in numerical_vars}\r\n",
    ")\r\n",
    "\r\n",
    "X_train.fillna(fillna_dict, inplace=True)\r\n",
    "X_train.to_csv(\"./results/splits/X_train.csv\", index=False)\r\n",
    "\r\n",
    "# impute test with values obtained from train\r\n",
    "X_test.fillna(fillna_dict, inplace=True)\r\n",
    "X_test.to_csv(\"./results/splits/X_test.csv\", index=False)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Imputation Strategies:\n",
      "Categorical: mode\n",
      "Numerical: median\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linearise Datasets\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "HYPER_max_pad = 15\r\n",
    "\r\n",
    "for dataset in (\"train\", \"test\"):\r\n",
    "    X = pd.read_csv(f\"./results/splits/X_{dataset}.csv\")\r\n",
    "\r\n",
    "    new_columns = list()\r\n",
    "    for i in range(HYPER_max_pad):\r\n",
    "        new_columns.extend([f\"{x}_{i}\" for x in X.columns])\r\n",
    "\r\n",
    "    grouped = X.groupby(X[\"Batch\"])\r\n",
    "\r\n",
    "    rows = list()\r\n",
    "    for k, group in grouped.groups.items():\r\n",
    "        # add row containing both real rows and zero rows\r\n",
    "        if len(group) > HYPER_max_pad:\r\n",
    "            group = group[:HYPER_max_pad]\r\n",
    "        X_sub = X.loc[group]\r\n",
    "\r\n",
    "        row = list()\r\n",
    "        for i in range(len(group)):\r\n",
    "            row.extend(X_sub.iloc[i].tolist())\r\n",
    "        for i in range(HYPER_max_pad - len(group)):\r\n",
    "            row.extend([0] * len(X.columns))\r\n",
    "        rows.append(row)\r\n",
    "\r\n",
    "    new_X = pd.DataFrame(data=rows, columns=new_columns)\r\n",
    "    new_X.to_csv(f\"./results/splits/X_{dataset}_padded_translated.csv\", index=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "bddf87d3d8ffc9ffe643d1c63ef98549d54d18316b6f1377644eea2ca2a14d20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}